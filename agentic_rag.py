from typing import List, Dict, Optional, Tuple
import json
from recursive_text_splitter import RecursiveTextSplitter
from knowledge_database import VectorDatabase
from chat import chat


class AgenticRAG:
    """
    å…·æœ‰åæ€èƒ½åŠ›çš„æ™ºèƒ½RAGç³»ç»Ÿ
    èƒ½å¤Ÿæ ¹æ®å›ç­”è´¨é‡è¿›è¡Œåæ€ï¼Œå¹¶é‡æ–°æœç´¢ç›¸å…³ä¿¡æ¯
    """
    
    def __init__(self, collection_name: str = "agentic_rag", max_iterations: int = 2):
        """
        åˆå§‹åŒ–Agentic RAGç³»ç»Ÿ
        
        Args:
            collection_name: å‘é‡æ•°æ®åº“é›†åˆåç§°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        self.db = VectorDatabase()
        self.collection_name = collection_name
        self.max_iterations = max_iterations
        self.conversation_history = []
        self.query_history = set()  # è®°å½•å·²ä½¿ç”¨çš„æŸ¥è¯¢è¯­å¥
        
    def setup_knowledge_base(self, documents: List[str], metadata: List[Dict] = None):
        """
        è®¾ç½®çŸ¥è¯†åº“
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            metadata: å¯é€‰çš„å…ƒæ•°æ®åˆ—è¡¨
        """
        # åˆ›å»ºé›†åˆ
        self.db.create_collection(self.collection_name, dimension=1024, drop_if_exists=True)
        
        # æ–‡æœ¬åˆ†å‰²
        splitter = RecursiveTextSplitter(chunk_size=500)
        all_chunks = []
        all_metadata = []
        
        for i, doc in enumerate(documents):
            chunks = splitter.split_text(doc)
            all_chunks.extend(chunks)
            
            # ä¸ºæ¯ä¸ªchunkæ·»åŠ å…ƒæ•°æ®
            for chunk in chunks:
                chunk_metadata = {"doc_id": i, "chunk_text": chunk}
                if metadata and i < len(metadata):
                    chunk_metadata.update(metadata[i])
                all_metadata.append(chunk_metadata)
        
        # æ’å…¥å‘é‡æ•°æ®åº“
        self.db.insert_documents(self.collection_name, all_chunks, all_metadata)
        print(f"çŸ¥è¯†åº“è®¾ç½®å®Œæˆï¼Œå…±æ’å…¥ {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
    
    def initial_search(self, query: str, limit: int = 5) -> List[Dict]:
        """
        åˆå§‹æœç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            limit: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = self.db.search(self.collection_name, query, limit=limit)
        self.query_history.add(query)  # è®°å½•æŸ¥è¯¢å†å²
        print("\n=== åˆå§‹æœç´¢ ===")
        print(f"æŸ¥è¯¢: {query}")
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
        return results
    
    def generate_initial_answer(self, query: str, search_results: List[Dict]) -> str:
        """
        åŸºäºåˆå§‹æœç´¢ç»“æœç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            search_results: æœç´¢ç»“æœ
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        context = "\n".join([f"æ–‡æ¡£{i+1}: {r['text']}" for i, r in enumerate(search_results)])
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚é‡è¦æé†’ï¼šè¯·ä¸¥æ ¼åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”ï¼Œä¸è¦æé€ æˆ–ç¼–é€ æ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºéœ€è¦æ›´å¤šä¿¡æ¯çš„æ–¹é¢ï¼Œä¸è¦è¿›è¡Œæ¨æµ‹æˆ–å‡è®¾ã€‚"},
            {"role": "user", "content": f"åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š\n\næ–‡æ¡£å†…å®¹ï¼š\n{context}\n\né—®é¢˜ï¼š{query}"}
        ]
        
        response = chat(messages)
        answer = response.choices[0].message.content
        print("\n=== åˆå§‹å›ç­” ===")
        print(answer)
        return answer
    
    def reflect_on_answer(self, query: str, answer: str, search_results: List[Dict]) -> Dict:
        """
        å¯¹å›ç­”è¿›è¡Œåæ€å’Œè¯„ä¼°
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            answer: ç”Ÿæˆçš„å›ç­”
            search_results: æœç´¢ç»“æœ
            
        Returns:
            åæ€ç»“æœå­—å…¸ï¼ŒåŒ…å«æ”¹è¿›å»ºè®®å’Œæœç´¢ç­–ç•¥
        """
        
        reflection_prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”çš„å®Œæ•´æ€§ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š

        ç”¨æˆ·é—®é¢˜ï¼š{query}
        å½“å‰å›ç­”ï¼š{answer}
        å·²æ£€ç´¢æ–‡æ¡£æ•°ï¼š{len(search_results)}

        è¯·è¯„ä¼°ï¼š
        1. å›ç­”æ˜¯å¦å®Œæ•´å›ç­”äº†ç”¨æˆ·é—®é¢˜
        2. å¦‚æœä¸å®Œæ•´ï¼Œéœ€è¦æœç´¢ä»€ä¹ˆä¿¡æ¯æ¥è¡¥å……
        3. ç”Ÿæˆå…·ä½“çš„è¯­ä¹‰æœç´¢æŸ¥è¯¢è¯­å¥ï¼ˆä¸æ˜¯å…³é”®è¯ï¼‰

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
        {{
            "is_complete": true/false,
            "missing_info": "å¦‚æœä¸å®Œæ•´ï¼Œæè¿°ç¼ºå°‘ä»€ä¹ˆä¿¡æ¯",
            "search_queries": ["å…·ä½“çš„æœç´¢æŸ¥è¯¢è¯­å¥1", "å…·ä½“çš„æœç´¢æŸ¥è¯¢è¯­å¥2"]
        }}

        æ³¨æ„ï¼š
        - search_queriesåº”è¯¥æ˜¯å®Œæ•´çš„é—®å¥æˆ–æè¿°ï¼Œä¸æ˜¯å•ä¸ªå…³é”®è¯
        - åªæœ‰å½“å›ç­”æ˜æ˜¾ä¸è¶³æ—¶æ‰è®¾ç½®is_completeä¸ºfalse
        - æœ€å¤šç”Ÿæˆ3ä¸ªæœç´¢æŸ¥è¯¢
        """
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”è´¨é‡åˆ†æå¸ˆã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ–‡æœ¬ã€‚"},
            {"role": "user", "content": reflection_prompt}
        ]
        
        response = chat(messages)
        reflection_text = response.choices[0].message.content.strip()
        
        # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        if reflection_text.startswith('```json'):
            reflection_text = reflection_text[7:]  # ç§»é™¤ ```json
        elif reflection_text.startswith('```'):
            reflection_text = reflection_text[3:]   # ç§»é™¤ ```
            
        if reflection_text.endswith('```'):
            reflection_text = reflection_text[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
            
        reflection_text = reflection_text.strip()
        
        # è§£æJSONå“åº”
        try:
            reflection = json.loads(reflection_text)
            
            # éªŒè¯JSONç»“æ„
            if not isinstance(reflection.get('is_complete'), bool):
                raise ValueError("is_complete must be boolean")
            
            if not isinstance(reflection.get('search_queries'), list):
                reflection['search_queries'] = []
                
        except (json.JSONDecodeError, Exception) as e:
            print(f"JSONè§£æé”™è¯¯: {e}")
            print(f"åŸå§‹å“åº”: {reflection_text}")
            # æä¾›é»˜è®¤ç»“æ„
            reflection = {
                "is_complete": False,
                "missing_info": "æ— æ³•è§£æåæ€ç»“æœ",
                "search_queries": []
            }
        
        print("\n=== åæ€ç»“æœ ===")
        print(f"å›ç­”æ˜¯å¦å®Œæ•´: {reflection['is_complete']}")
        print(f"ç¼ºå°‘ä¿¡æ¯: {reflection.get('missing_info', 'N/A')}")
        print(f"å»ºè®®æœç´¢æŸ¥è¯¢: {reflection['search_queries']}")
        
        return reflection
    
    def refined_search(self, reflection: Dict, previous_results: List[Dict]) -> List[Dict]:
        """
        åŸºäºåæ€ç»“æœè¿›è¡Œç²¾ç»†åŒ–æœç´¢
        
        Args:
            reflection: åæ€ç»“æœ
            previous_results: ä¹‹å‰çš„æœç´¢ç»“æœ
            
        Returns:
            æ–°çš„æœç´¢ç»“æœ
        """
        search_queries = reflection.get('search_queries', [])
        previous_texts = {r['text'] for r in previous_results}
        all_new_results = []
        
        print("\n=== ç²¾ç»†åŒ–æœç´¢ ===")
        
        if not search_queries:
            print("æ²¡æœ‰å»ºè®®çš„æœç´¢æŸ¥è¯¢ï¼Œè·³è¿‡ç²¾ç»†åŒ–æœç´¢")
            return []
        
        # è¿‡æ»¤æ‰å·²ç»ä½¿ç”¨è¿‡çš„æŸ¥è¯¢
        new_queries = []
        for query in search_queries:
            if query and query not in self.query_history:
                new_queries.append(query)
                self.query_history.add(query)
            else:
                print(f"è·³è¿‡é‡å¤æŸ¥è¯¢: {query}")
        
        if not new_queries:
            print("æ‰€æœ‰å»ºè®®çš„æŸ¥è¯¢éƒ½å·²ä½¿ç”¨è¿‡ï¼Œè·³è¿‡æœç´¢")
            return []
        
        # å¯¹æ¯ä¸ªæ–°æŸ¥è¯¢è¿›è¡Œè¯­ä¹‰æœç´¢
        for query in new_queries:
            print(f"è¯­ä¹‰æœç´¢: {query}")
            new_results = self.db.search(self.collection_name, query, limit=3)
            
            # å»é‡ï¼šç§»é™¤ä¸ä¹‹å‰ç»“æœé‡å¤çš„å†…å®¹
            unique_results = []
            for result in new_results:
                if result['text'] not in previous_texts:
                    unique_results.append(result)
                    previous_texts.add(result['text'])
            
            all_new_results.extend(unique_results)
            print(f"  æ‰¾åˆ° {len(unique_results)} ä¸ªæ–°æ–‡æ¡£")
        
        print(f"æ€»å…±æ‰¾åˆ° {len(all_new_results)} ä¸ªæ–°çš„ç›¸å…³æ–‡æ¡£")
        return all_new_results
    
    def generate_improved_answer(self, query: str, all_search_results: List[Dict], iteration: int) -> str:
        """
        åŸºäºæ‰€æœ‰æœç´¢ç»“æœç”Ÿæˆæ”¹è¿›çš„å›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            all_search_results: æ‰€æœ‰æœç´¢ç»“æœ
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            
        Returns:
            æ”¹è¿›çš„å›ç­”
        """
        context = "\n".join([f"æ–‡æ¡£{i+1}: {r['text']}" for i, r in enumerate(all_search_results)])
        
        messages = [
            {"role": "system", "content": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¿™æ˜¯ç¬¬{iteration}æ¬¡è¿­ä»£ä¼˜åŒ–ã€‚è¯·åŸºäºæä¾›çš„æ‰€æœ‰æ–‡æ¡£å†…å®¹ç»™å‡ºæœ€å…¨é¢ã€å‡†ç¡®çš„å›ç­”ã€‚é‡è¦æé†’ï¼šè¯·ä¸¥æ ¼åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”ï¼Œä¸è¦æé€ æˆ–ç¼–é€ æ–‡æ¡£ä¸­ä¸å­˜åœ¨çš„ä¿¡æ¯ï¼Œä¸è¦è¿›è¡Œæ— æ ¹æ®çš„æ¨æµ‹æˆ–å‡è®¾ã€‚"},
            {"role": "user", "content": f"åŸºäºä»¥ä¸‹æ‰€æœ‰æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š\n\næ–‡æ¡£å†…å®¹ï¼š\n{context}\n\né—®é¢˜ï¼š{query}"}
        ]
        
        response = chat(messages)
        improved_answer = response.choices[0].message.content
        print(f"\n=== ç¬¬{iteration}æ¬¡æ”¹è¿›å›ç­” ===")
        print(improved_answer)
        return improved_answer
    
    def query(self, user_query: str) -> Dict:
        """
        æ‰§è¡ŒAgentic RAGæŸ¥è¯¢æµç¨‹
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            åŒ…å«æœ€ç»ˆç­”æ¡ˆå’Œå¤„ç†è¿‡ç¨‹çš„å­—å…¸
        """
        print(f"\n{'='*60}")
        print("å¼€å§‹Agentic RAGæŸ¥è¯¢æµç¨‹")
        print(f"ç”¨æˆ·é—®é¢˜: {user_query}")
        print(f"{'='*60}")
        
        # é‡ç½®æŸ¥è¯¢å†å²
        self.query_history.clear()
        
        # 1. åˆå§‹æœç´¢å’Œå›ç­”
        search_results = self.initial_search(user_query)
        current_answer = self.generate_initial_answer(user_query, search_results)
        
        all_search_results = search_results.copy()
        iteration_history = []
        
        # 2. è¿­ä»£æ”¹è¿›æµç¨‹
        for iteration in range(1, self.max_iterations + 1):
            print(f"\n--- ç¬¬{iteration}æ¬¡è¿­ä»£ ---")
            
            # åæ€å½“å‰å›ç­”
            reflection = self.reflect_on_answer(user_query, current_answer, all_search_results)
            
            # è®°å½•è¿­ä»£å†å²
            iteration_info = {
                "iteration": iteration,
                "answer": current_answer,
                "reflection": reflection,
                "search_results_count": len(all_search_results)
            }
            iteration_history.append(iteration_info)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æ”¹è¿›
            if reflection.get('is_complete', False):
                print("åæ€ç»“æœæ˜¾ç¤ºå›ç­”å·²å®Œæ•´ï¼Œåœæ­¢è¿­ä»£")
                break
            
            # è¿›è¡Œç²¾ç»†åŒ–æœç´¢
            new_results = self.refined_search(reflection, all_search_results)
            
            if not new_results:
                print("æ²¡æœ‰æ‰¾åˆ°æ–°çš„ç›¸å…³ä¿¡æ¯ï¼Œåœæ­¢è¿­ä»£")
                break
            
            # åˆå¹¶æœç´¢ç»“æœ
            all_search_results.extend(new_results)
            
            # ç”Ÿæˆæ”¹è¿›çš„å›ç­”
            current_answer = self.generate_improved_answer(user_query, all_search_results, iteration)
        
        # 3. è¿”å›æœ€ç»ˆç»“æœ
        final_result = {
            "query": user_query,
            "final_answer": current_answer,
            "total_search_results": len(all_search_results),
            "iterations": len(iteration_history),
            "iteration_history": iteration_history,
            "all_search_results": all_search_results
        }
        
        print(f"\n{'='*60}")
        print("Agentic RAGæŸ¥è¯¢å®Œæˆ")
        print(f"æ€»è¿­ä»£æ¬¡æ•°: {len(iteration_history)}")
        print(f"æœ€ç»ˆæ£€ç´¢æ–‡æ¡£æ•°: {len(all_search_results)}")
        print(f"{'='*60}")
        
        return final_result


def build_agentic_rag_demo():
    """æ„å»ºå’Œæµ‹è¯•Agentic RAGç³»ç»Ÿ"""
    
    # åˆ›å»ºAgentic RAGå®ä¾‹
    agentic_rag = AgenticRAG(collection_name="agentic_demo", max_iterations=2)
    
    # å‡†å¤‡çŸ¥è¯†åº“æ–‡æ¡£
    documents = [
        """
        äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§°AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œ
        å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€
        è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œ
        åº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ã€‚
        """,
        """
        æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚å®ƒæ˜¯ä¸€ç§é€šè¿‡ç®—æ³•è§£ææ•°æ®ã€ä»ä¸­å­¦ä¹ ï¼Œç„¶åå¯¹çœŸå®ä¸–ç•Œ
        ä¸­çš„äº‹ä»¶åšå‡ºå†³ç­–å’Œé¢„æµ‹çš„æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ä¸ºè§£å†³ç‰¹å®šä»»åŠ¡ã€ç¡¬ç¼–ç çš„è½¯ä»¶ç¨‹åºä¸åŒï¼Œæœºå™¨å­¦ä¹ 
        æ˜¯ç”¨å¤§é‡çš„æ•°æ®æ¥"è®­ç»ƒ"ï¼Œé€šè¿‡å„ç§ç®—æ³•ä»æ•°æ®ä¸­å­¦ä¹ å¦‚ä½•å®Œæˆä»»åŠ¡ã€‚å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬
        çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºå’Œç¥ç»ç½‘ç»œç­‰ã€‚
        """,
        """
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒæ¨¡æ‹Ÿäººè„‘çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œé€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚
        æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
        ä¸»è¦ç”¨äºå›¾åƒå¤„ç†ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ä¸»è¦ç”¨äºåºåˆ—æ•°æ®å¤„ç†ï¼Œ
        è€ŒTransformeræ¶æ„åˆ™åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§å˜åŒ–ã€‚
        """,
        """
        è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼Œç®€ç§°NLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åº”ç”¨é¢†åŸŸï¼Œ
        å®ƒç ”ç©¶å¦‚ä½•è®©è®¡ç®—æœºç†è§£ã€å¤„ç†å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚NLPçš„ä¸»è¦ä»»åŠ¡åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€
        æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPTã€BERT
        ç­‰çš„å‡ºç°ï¼Œå¤§å¤§æ¨åŠ¨äº†NLPæŠ€æœ¯çš„å‘å±•ã€‚
        """,
        """
        è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒè‡´åŠ›äºè®©è®¡ç®—æœºèƒ½å¤Ÿ"çœ‹æ‡‚"å›¾åƒå’Œè§†é¢‘ã€‚
        è®¡ç®—æœºè§†è§‰çš„ä¸»è¦ä»»åŠ¡åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€äººè„¸è¯†åˆ«ç­‰ã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œ
        ç‰¹åˆ«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œçš„å‘å±•ï¼Œä½¿å¾—è®¡ç®—æœºè§†è§‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¶Šäº†äººç±»çš„è¡¨ç°ã€‚
        """,
        """
        äººå·¥æ™ºèƒ½çš„åº”ç”¨é¢†åŸŸéå¸¸å¹¿æ³›ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šåŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ã€è‡ªåŠ¨é©¾é©¶ã€æ™ºèƒ½æ¨èã€
        è¯­éŸ³åŠ©æ‰‹ã€æœºå™¨ç¿»è¯‘ã€æ¸¸æˆAIç­‰ã€‚åœ¨åŒ»ç–—é¢†åŸŸï¼ŒAIçš„å…·ä½“åº”ç”¨åŒ…æ‹¬ï¼šåŒ»å­¦å½±åƒåˆ†æã€ç–¾ç—…è¯Šæ–­ã€
        è¯ç‰©å‘ç°ã€ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆã€æ‰‹æœ¯æœºå™¨äººã€å¥åº·ç›‘æµ‹ã€ç”µå­ç—…å†åˆ†æã€åŸºå› åˆ†æç­‰ã€‚
        éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒAIæ­£åœ¨æ”¹å˜å„ä¸ªè¡Œä¸šçš„å·¥ä½œæ–¹å¼ï¼Œæé«˜æ•ˆç‡ï¼Œåˆ›é€ æ–°çš„å•†ä¸šæ¨¡å¼ã€‚
        åŒæ—¶ï¼ŒAIçš„å‘å±•ä¹Ÿå¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚å°±ä¸šå½±å“ã€éšç§ä¿æŠ¤ã€ç®—æ³•åè§ç­‰é—®é¢˜ï¼Œéœ€è¦ç¤¾ä¼šå„ç•Œå…±åŒå…³æ³¨å’Œè§£å†³ã€‚
        """,
        """
        äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—å¥åº·é¢†åŸŸçš„åº”ç”¨æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚å…·ä½“åº”ç”¨åŒ…æ‹¬ï¼š
        1. åŒ»å­¦å½±åƒåˆ†æï¼šåˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯åˆ†æXå…‰ã€CTã€MRIç­‰åŒ»å­¦å½±åƒï¼Œè¾…åŠ©åŒ»ç”Ÿè¯Šæ–­è‚¿ç˜¤ã€éª¨æŠ˜ç­‰ç–¾ç—…ã€‚
        2. è¯ç‰©ç ”å‘ï¼šé€šè¿‡æœºå™¨å­¦ä¹ ç®—æ³•åŠ é€Ÿæ–°è¯å‘ç°è¿‡ç¨‹ï¼Œé¢„æµ‹è¯ç‰©åˆ†å­çš„æœ‰æ•ˆæ€§å’Œå®‰å…¨æ€§ã€‚
        3. ä¸ªæ€§åŒ–æ²»ç–—ï¼šåŸºäºæ‚£è€…çš„åŸºå› ä¿¡æ¯ã€ç—…å²å’Œç”Ÿç†æ•°æ®ï¼Œä¸ºæ‚£è€…åˆ¶å®šä¸ªæ€§åŒ–çš„æ²»ç–—æ–¹æ¡ˆã€‚
        4. å¥åº·ç›‘æµ‹ï¼šåˆ©ç”¨å¯ç©¿æˆ´è®¾å¤‡å’Œä¼ æ„Ÿå™¨ï¼Œå®æ—¶ç›‘æµ‹æ‚£è€…çš„ç”Ÿå‘½ä½“å¾å’Œå¥åº·çŠ¶å†µã€‚
        5. æ‰‹æœ¯æœºå™¨äººï¼šååŠ©åŒ»ç”Ÿè¿›è¡Œç²¾å¯†æ‰‹æœ¯ï¼Œæé«˜æ‰‹æœ¯ç²¾åº¦å’Œå®‰å…¨æ€§ã€‚
        6. ç”µå­ç—…å†åˆ†æï¼šè‡ªåŠ¨æå–å’Œåˆ†æç”µå­ç—…å†ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œè¾…åŠ©ä¸´åºŠå†³ç­–ã€‚
        è¿™äº›åº”ç”¨å¤§å¤§æé«˜äº†åŒ»ç–—æœåŠ¡çš„è´¨é‡å’Œæ•ˆç‡ï¼Œä¸ºæ‚£è€…æä¾›æ›´å¥½çš„åŒ»ç–—ä½“éªŒã€‚
        """
    ]
    
    # å‡†å¤‡å…ƒæ•°æ®
    metadata = [
        {"category": "AIåŸºç¡€", "topic": "äººå·¥æ™ºèƒ½å®šä¹‰", "difficulty": "åˆçº§"},
        {"category": "æœºå™¨å­¦ä¹ ", "topic": "æœºå™¨å­¦ä¹ æ¦‚è¿°", "difficulty": "ä¸­çº§"},
        {"category": "æ·±åº¦å­¦ä¹ ", "topic": "æ·±åº¦å­¦ä¹ æŠ€æœ¯", "difficulty": "é«˜çº§"},
        {"category": "NLP", "topic": "è‡ªç„¶è¯­è¨€å¤„ç†", "difficulty": "ä¸­çº§"},
        {"category": "è®¡ç®—æœºè§†è§‰", "topic": "å›¾åƒå¤„ç†", "difficulty": "ä¸­çº§"},
        {"category": "AIåº”ç”¨", "topic": "åº”ç”¨åœºæ™¯", "difficulty": "åˆçº§"},
        {"category": "åŒ»ç–—AI", "topic": "åŒ»ç–—å¥åº·åº”ç”¨", "difficulty": "ä¸­çº§"}
    ]
    
    # è®¾ç½®çŸ¥è¯†åº“
    agentic_rag.setup_knowledge_base(documents, metadata)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿå®ƒæœ‰å“ªäº›ä¸»è¦åº”ç”¨é¢†åŸŸï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸæœ‰å“ªäº›å…·ä½“åº”ç”¨ï¼Ÿ"
    ]
    
    # æ‰§è¡Œæµ‹è¯•
    for i, query in enumerate(test_queries, 1):
        print(f"\n\nğŸ” æµ‹è¯•æŸ¥è¯¢ {i}: {query}")
        agentic_rag.query(query)


if __name__ == "__main__":
    build_agentic_rag_demo()
